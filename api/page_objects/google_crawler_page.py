import json
import os.path
from contextlib import asynccontextmanager
from bs4 import BeautifulSoup
from typing import Dict
from typing import Set
from pyppeteer.errors import PageError, TimeoutError, NetworkError
from pyppeteer_stealth import stealth
from urllib.request import urlopen
from api.config import DEFAULT_SEARCH_ENGINE_URL, PACKETSTREAM_USERNAME, PACKETSTREAM_PASSWORD, PACKETSTREAM_PROXY_DOMAIN, PACKETSTREAM_HTTPS_PORT, PACKETSTREAM_HTTP_PORT
from pyppeteer import launch
from asyncio import TimeoutError as ForcedTimeoutError
from os import path, makedirs
import asyncio
import logging
import fitz
import pymupdf
import datetime
import humanize
from pymupdf import utils

from api.util.ManifestUtils import Manifest, ManifestEntry

logger = logging.getLogger('Google Page')

async def extract_urls(page) -> Set[str]:
    try:
        await page.waitForSelector('body')
        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")
        list_anchors = []
        for anchor in soup.find_all("a", {'class': 'gs-title'}, href=True):
            list_anchors.append(anchor['href'])

        return set(list_anchors)
    except Exception as e:
        logger.error(f'Error extracting URLs: {e}')
        return set()


async def can_paginate(page):
    elements = await page.xpath('//div[@class="gsc-cursor-page"]')
    return len(elements) > 1


async def has_results(page):
    ele = await page.querySelector('div[id="resInfo-0"]')
    result = await page.evaluate('(element) => element.textContent', ele)
    return len(result.strip()) != 0

def handle_request(request):
    asyncio.create_task (intercept_request(request))

async def intercept_request (request):
    blocked_resource_types = [
        'beacon',
        'csp_report',
        'font',
        'image',
        'imageset',
        'media',
        'object',
        'texttrack',
    ]

    if request.resourceType in blocked_resource_types:
        logger.debug (f'Blocked type: {request.resourceType}, url: {request.url}')
        await request.abort()
    else:
        logger.debug(f'Making request: {request.resourceType}: {request.url}')
        await request.continue_()


async def extract_text_from_pdf(pdf_path: str):
    try:
        if os.path.exists(pdf_path):
            file_size_in_bytes = os.path.getsize(pdf_path)
            file_size = humanize.naturalsize(file_size_in_bytes)
            logger.info (f'Extracting text from PDF may take some time depending on the size of the PDF. Size: {file_size}')
            doc = fitz.open(pdf_path)
            text_file_path = path.splitext(pdf_path)[0] + '.txt'
            with open(text_file_path, 'w') as text_file:
                for page_num in range(doc.page_count):
                    page = doc.load_page(page_num)
                    text_file.write(page.get_text())
            logger.info(f"Text extracted and saved to {text_file_path}")
    except Exception as e:
        logger.error(f"Error extracting text from {pdf_path}: {e}")


async def create_manifest_for_urls(urls: Set, category: str):
    manifest_file_path = path.join('./tmp', category, 'manifest.txt')
    manifest_map = {}
    with open(manifest_file_path, 'w') as manifest_file:
        file_number = 1
        for url in urls:
            manifest_map[url] = f"{file_number}"
            manifest_file.write(f"{file_number} -> {url}\n")
            file_number += 1
    return manifest_map

def insert_date_to_pdf(pdf_path):
    date_now = datetime.datetime.now().strftime("%I:%M%p on %B %d, %Y")
    text_to_write = f'''
    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n
                                Generated by VDD Crawler at {date_now} 
    '''
    doc = pymupdf.open(pdf_path)
    utils.insert_page(doc, 0, text_to_write)
    doc.save(pdf_path, incremental=True, encryption=0)
    doc.close()

async def write_pdf_file(url, file_path):
    try:
        pdf_content = urlopen(url)
        with open(file_path, 'wb') as f:
            f.write(pdf_content.read())
        insert_date_to_pdf(file_path)
        logger.info(f'Converted page: {url} to PDF {file_path}')
    except Exception as e:
        logger.error(f'Error writing PDF file {file_path}: {e}')

async def to_textised_pdf(page, url, pdf_path):
    logger.debug ('Textising the content...')
    await page.goto('https://www.textise.net/')
    await page.type('input[name="in"]', url)
    await page.keyboard.press('Enter')
    await page.waitForSelector('div[textise="block"]')
    logger.debug('Done textising')

    await to_pdf(page, pdf_path)
    logger.info(f'Converted: {url} to PDF {pdf_path}')

async def to_pdf(page, pdf_path):
    try:
        await page.emulateMedia('screen')
        await page.pdf(path=pdf_path)
        insert_date_to_pdf(pdf_path)
    except Exception as e:
        logger.error(f'Failed creating PDF: {e}')

async def dump_markup(page, dump_path):
    try:
        content = await page.content()
        with open(dump_path, 'w') as f:
            f.write(content)
    except Exception as e:
        logger.info (f'Error dumping markup: {e}')

async def get_browser_with_proxy():
    proxy = f'https://{PACKETSTREAM_PROXY_DOMAIN}:{PACKETSTREAM_HTTPS_PORT}'
    _browser_with_proxy = await launch({
            'executablePath': '/usr/bin/chromium',
            'headless': True,
            'ignoreHTTPSErrors': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage', f'--proxy-server={proxy}', '--ignore-certificate-errors', '--lang=en-IN,en'],
    })
    return _browser_with_proxy


async def get_browser():
    _browser = await launch({
            'executablePath': '/usr/bin/chromium',
            'headless': True,
            'ignoreHTTPSErrors': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage','--ignore-certificate-errors', '--lang=en-IN,en'],
        })
    return _browser


async def perform_google_search(page, search_term: str, working_dir, num_pages_to_crawl, search_url=DEFAULT_SEARCH_ENGINE_URL):
    search_page_urls = set()
    await stealth(page)
    page_number = 1

    try:
        await page.goto(search_url)
        await page.type('input[name="search"]', search_term)
        await page.keyboard.press('Enter')
        await page.waitForSelector('div[id="resInfo-0"]')
        search_page_urls.update(await extract_urls(page))
        pdf_path = working_dir + f'/google_results_{page_number}.pdf'
        await to_pdf(page, pdf_path)

        page_number = 2
        while page_number <= num_pages_to_crawl:
            if await can_paginate(page):
                logger.debug(f'Navigating to page {page_number}...')
                await page.waitForXPath(f'//div[@class="gsc-cursor-page"][{page_number}]')
                elements_to_click = await page.xpath(f'//div[@role="link"][{page_number - 1}]')
                try:
                    await asyncio.gather(
                        elements_to_click[0].click(),
                        page.waitForNavigation(),
                    )
                    await asyncio.sleep(5)
                    on_page = await page.querySelectorEval('.gsc-cursor-current-page', 'node => node.innerText')
                    logger.debug(f"On page {on_page}")
                except TimeoutError as e:
                    logger.error(f'Timed-out waiting to confirm, proceeding without confirmation: {e}')
                    #await dump_markup(page, f'{working_dir}/markup_dump_timeout.html')
                except Exception as e:
                    logger.error(f'Error occurred navigating: {e}')
                    #await dump_markup(page, f'{working_dir}/markup_dump_error.html')
                    #await to_pdf(page, f'{working_dir}/google_navigation_error.pdf')

                search_page_urls.update(await extract_urls(page))
                logger.debug('Generating google search results PDF...')
                pdf_path = working_dir + f'/google_results_{page_number}.pdf'
                await to_pdf(page, pdf_path)
                page_number += 1
            else:
                break
    except TimeoutError as te:
        logger.error(f'Timeout error occurred while performing search: {te}')
        pdf_path = working_dir + '/google_error.pdf'
        await to_pdf(page, pdf_path)
    except Exception as e:
        logger.error(f'Error occurred searching Google: {e}')
        #await dump_markup(page, f'{working_dir}/markup_dump_search_error.html')
    logger.info(f'Found {len(search_page_urls)} URLs')
    return search_page_urls


def create_final_manifest(manifest, path):
    manifest_file_path = f'{path}/manifest.json'
    with open (manifest_file_path, 'w') as manifest_file:
        manifest_file.write(json.dumps(manifest, default=vars))


class CrawlerPage:

    @asynccontextmanager
    async def new_intercepted_page(self):
        browser = await get_browser_with_proxy()
        page = await browser.newPage()
        await page.setRequestInterception(True)
        page.on('request', handle_request)
        await page.authenticate({"username":f"{PACKETSTREAM_USERNAME}", "password":f"{PACKETSTREAM_PASSWORD}"})
        try:
            yield page
        finally:
            await browser.close()

    @asynccontextmanager
    async def new_page(self):
        browser = await get_browser()
        page = await browser.newPage()
        try:
            yield page
        finally:
            try:
                await browser.close()
            except Exception as e:
                logger.info('Error while closing browser.', e)

    async def prepare_pdfs(self, urls_manifest: Dict, working_dir):
        manifest = Manifest()
        for url, file_name in urls_manifest.items():
            manifest_entry = ManifestEntry(url, file_name)
            file_path = f"{working_dir}/{file_name}.pdf"
            if url.lower().endswith('.pdf'):
                try:
                    logger.info(f'Processing {file_name} -> {url}')
                    await asyncio.wait_for(write_pdf_file(url, file_path), timeout=120)
                    manifest_entry.set_status(True)
                except ForcedTimeoutError as e:
                    logger.error('PDF either too large or it is taking too long to load. Skipping.')
                except Exception as e:
                    logger.error(f'Error occurred while downloading PDF: {e}')
            else:
                async with self.new_intercepted_page() as page:
                    logger.info(f'Processing {file_name} -> {url}')
                    await stealth(page)
                    try:
                        pdf_path = path.join(working_dir, f'{file_name}.pdf')
                        logger.info(f'Loading page :{url}')
                        await page.goto(url, {'waituntil': 'networkidle0', 'timeout': 120000})
                        logger.info(f'Converting to PDF')
                        await asyncio.wait_for(to_pdf(page, pdf_path), timeout=90)
                        manifest_entry.set_status(True)
                    except ForcedTimeoutError as e:
                        logger.error('Page taking too long to load. Skipping')
                    except PageError as e:
                        logger.error(f'Page error converting page to PDF: {url}: {e}')
                    except NetworkError as e:
                        logger.error(f'Network error converting page to PDF: {url}: {e}')
            manifest.add(manifest_entry)
            await extract_text_from_pdf(file_path)
        return manifest


    async def search_and_download(self, search_term: str,
        num_of_results_pages_to_scrape: int,
        category: str,
        search_url=DEFAULT_SEARCH_ENGINE_URL):

        dir_path = f'./tmp/{category}'
        makedirs(dir_path, exist_ok=True)

        try:
            async with self.new_intercepted_page() as page:
                logger.info(f'Using search term: {search_term}')
                search_page_urls = await perform_google_search(page, search_term, dir_path, num_of_results_pages_to_scrape, search_url=search_url)

            logger.info("Preparing manifest...")
            manifest_map = await create_manifest_for_urls(search_page_urls, category)
            logger.info("Downloading...")
            manifest = await self.prepare_pdfs(manifest_map, dir_path)
            create_final_manifest(manifest, dir_path)
        except Exception as e:
            logger.error('Error occurred in search and download', e)
