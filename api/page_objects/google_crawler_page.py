from contextlib import asynccontextmanager
from bs4 import BeautifulSoup
from typing import Dict
from typing import Set
from pyppeteer.errors import PageError, TimeoutError, NetworkError
from pyppeteer_stealth import stealth
from urllib.request import urlopen
from api.config import DEFAULT_SEARCH_ENGINE_URL, PACKETSTREAM_PROXY_URL, PACKETSTREAM_USERNAME, PACKETSTREAM_PASSWORD
from pyppeteer import launch
from asyncio import TimeoutError as ForcedTimeoutError
from os import path, makedirs
import asyncio
import logging
import fitz
import pymupdf
import datetime
from pymupdf import utils

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("crawler")

async def extract_urls(page) -> Set[str]:
    try:
        await page.waitForSelector('body')
        content = await page.content()
        soup = BeautifulSoup(content, "html.parser")
        list_anchors = []
        for anchor in soup.find_all("a", {'class': 'gs-title'}, href=True):
            list_anchors.append(anchor['href'])

        return set(list_anchors)
    except Exception as e:
        logger.error(f'Error extracting URLs: {e}')
        return set()


async def can_paginate(page):
    elements = await page.xpath('//div[@class="gsc-cursor-page"]')
    return len(elements) > 1


async def has_results(page):
    ele = await page.querySelector('div[id="resInfo-0"]')
    result = await page.evaluate('(element) => element.textContent', ele)
    return len(result.strip()) != 0

def handle_request(request):
    asyncio.create_task (intercept_request(request))

async def intercept_request (request):
    blocked_resource_types = [
        'beacon',
        'csp_report',
        'font',
        'image',
        'imageset',
        'media',
        'object',
        'texttrack',
    ]

    if request.resourceType in blocked_resource_types:
        logger.debug (f'Blocked type: {request.resourceType}, url: {request.url}')
        await request.abort()
    else:
        logger.debug(f'Making request: {request.resourceType}: {request.url}')
        await request.continue_()


async def extract_text_from_pdf(pdf_path: str):
    try:
        doc = fitz.open(pdf_path)
        text_file_path = path.splitext(pdf_path)[0] + '.txt'
        with open(text_file_path, 'w') as text_file:
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                text_file.write(page.get_text())
        logger.info(f"Text extracted and saved to {text_file_path}")
    except Exception as e:
        logger.error(f"Error extracting text from {pdf_path}: {e}")


async def create_manifest_for_urls(urls: Set, category: str):
    manifest_file_path = path.join('./tmp', category, 'manifest.txt')
    manifest_map = {}
    with open(manifest_file_path, 'w') as manifest_file:
        file_number = 1
        for url in urls:
            manifest_map[url] = f"{file_number}"
            manifest_file.write(f"{file_number} -> {url}\n")
            file_number += 1
    return manifest_map

def insert_date_to_pdf(pdf_path):
    date_now = datetime.datetime.now().strftime("%I:%M%p on %B %d, %Y")
    text_to_write = f'''
    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n
                                Generated by VDD Crawler at {date_now} 
    '''
    doc = pymupdf.open(pdf_path)
    utils.insert_page(doc, 0, text_to_write)
    doc.save(pdf_path, incremental=True, encryption=0)
    doc.close()

async def write_pdf_file(url, file_path):
    try:
        pdf_content = urlopen(url)
        with open(file_path, 'wb') as f:
            f.write(pdf_content.read())
        insert_date_to_pdf(file_path)
        logger.info(f'Converted page: {url} to PDF {file_path}')
    except Exception as e:
        logger.error(f'Error writing PDF file {file_path}: {e}')

async def to_textised_pdf(page, url, pdf_path):
    logger.info ('Textising the content...')
    await page.goto('https://www.textise.net/')
    await page.type('input[name="in"]', url)
    await page.keyboard.press('Enter')
    await page.waitForSelector('div[textise="block"]')
    logger.info('Done textising')

    await to_pdf(page, pdf_path)
    logger.info(f'Converted: {url} to PDF {pdf_path}')

async def to_pdf(page, pdf_path):
    try:
        await page.emulateMedia('screen')
        await page.pdf(path=pdf_path)
        insert_date_to_pdf(pdf_path)
    except Exception as e:
        logger.error(f'Failed creating PDF: {e}')

async def dump_markup(page, dump_path):
    try:
        content = await page.content()
        with open(dump_path, 'w') as f:
            f.write(content)
    except Exception as e:
        logger.info (f'Error dumping markup: {e}')

async def get_browser_with_proxy():
    _browser_with_proxy = await launch({
            'executablePath': '/usr/bin/chromium',
            'headless': True,
            'ignoreHTTPSErrors': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage', f'--proxy-server={PACKETSTREAM_PROXY_URL}', '--ignore-certificate-errors', '--lang=en-IN,en'],
    })
    return _browser_with_proxy


async def get_browser():
    _browser = await launch({
            'executablePath': '/usr/bin/chromium',
            'headless': True,
            'ignoreHTTPSErrors': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage','--ignore-certificate-errors', '--lang=en-IN,en'],
        })
    return _browser


async def perform_google_search(page, search_term: str, working_dir, num_pages_to_crawl, search_url=DEFAULT_SEARCH_ENGINE_URL):
    search_page_urls = set()
    await stealth(page)
    page_number = 1

    try:
        await page.goto(search_url)
        await page.type('input[name="search"]', search_term)
        await page.keyboard.press('Enter')
        await page.waitForSelector('div[id="resInfo-0"]')
        search_page_urls.update(await extract_urls(page))
        logger.info(f'Found {len(search_page_urls)} URLs')
        pdf_path = working_dir + f'/google_results_{page_number}.pdf'
        await to_pdf(page, pdf_path)

        page_number = 2
        while page_number <= num_pages_to_crawl:
            if await can_paginate(page):
                logger.info(f'Navigating to page {page_number}...')
                await page.waitForXPath('//div[@class="gsc-cursor-page"][1]')
                elements_to_click = await page.xpath(f'//div[@role="link"][{page_number - 1}]')
                try:
                    await asyncio.gather(
                        page.waitForNavigation(),
                        elements_to_click[0].click()
                    )
                except TimeoutError as e:
                    logger.info(f'Timed-out waiting to confirm, proceeding without confirmation: {e}')
                    #await dump_markup(page, f'{working_dir}/markup_dump_timeout.html')
                except Exception as e:
                    logger.info(f'Error occurred navigating: {e}')
                    #await dump_markup(page, f'{working_dir}/markup_dump_error.html')
                    #await to_pdf(page, f'{working_dir}/google_navigation_error.pdf')

                search_page_urls.update(await extract_urls(page))
                logger.info(f'Found {len(search_page_urls)} URLs')
                logger.info('Generating google search results PDF...')
                pdf_path = working_dir + f'/google_results_{page_number}.pdf'
                await to_pdf(page, pdf_path)
                logger.info('PDF Generated')
                page_number += 1
            else:
                break
    except TimeoutError as te:
        logger.error(f'Timeout error occurred while performing search: {te}')
        pdf_path = working_dir + '/google_error.pdf'
        await to_pdf(page, pdf_path)
    except Exception as e:
        logger.error(f'Error occurred searching Google: {e}')
    return search_page_urls


class CrawlerPage:

    @asynccontextmanager
    async def new_intercepted_page(self):
        browser = await get_browser_with_proxy()
        page = await browser.newPage()
        await page.setRequestInterception(True)
        page.on('request', handle_request)
        await page.authenticate({"username":f"{PACKETSTREAM_USERNAME}", "password":f"{PACKETSTREAM_PASSWORD}"})
        try:
            yield page
        finally:
            await browser.close()

    @asynccontextmanager
    async def new_page(self):
        browser = await get_browser()
        page = await browser.newPage()
        try:
            yield page
        finally:
            try:
                await browser.close()
            except Exception as e:
                logger.info('Error while closing browser.', e)

    async def prepare_pdfs(self, urls_manifest: Dict, working_dir):
        for url, file_name in urls_manifest.items():
            file_path = f"{working_dir}/{file_name}.pdf"
            if url.lower().endswith('.pdf'):
                logger.info(f'Downloading PDF: {url} to {file_name}')
                await asyncio.wait_for(write_pdf_file(url, file_path), timeout=90)
                logger.info(f'Downloaded PDF: {url} to {file_path}')
            else:
                async with self.new_page() as page:
                    logger.info(f'Converting page: {url} to PDF {file_name}')
                    await stealth(page)
                    try:
                        pdf_path = path.join(working_dir, f'{file_name}.pdf')
                        await asyncio.wait_for(to_textised_pdf(page, url, pdf_path), timeout=90)
                    except ForcedTimeoutError as e:
                        logger.error('Page either too large or is taking too long to load. Skipping')
                    except PageError as e:
                        logger.error(f'Page error converting page to PDF: {url}: {e}')
                    except NetworkError as e:
                        logger.error(f'Network error converting page to PDF: {url}: {e}')

            await extract_text_from_pdf(file_path)

    async def search_and_download(self, search_term: str,
        num_of_results_pages_to_scrape: int,
        category: str,
        search_url=DEFAULT_SEARCH_ENGINE_URL):

        dir_path = f'./tmp/{category}'
        makedirs(dir_path, exist_ok=True)

        try:
            async with self.new_intercepted_page() as page:
                logger.info(f'Using search term: {search_term}')
                search_page_urls = await perform_google_search(page, search_term, dir_path, num_of_results_pages_to_scrape, search_url=search_url)

            logger.info("Preparing manifest...")
            manifest_map = await create_manifest_for_urls(search_page_urls, category)
            logger.info("Manifest created")
            logger.info("System attempting to create PDF and TXT files out of the extracted URLs, this may take a while...")
            await self.prepare_pdfs(manifest_map, dir_path)
            logger.info("PDF and TXT extraction complete.")
        except Exception as e:
            logger.error('Error occurred in search and download', e)
